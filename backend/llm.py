from langchain_core.prompts import (
    HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate
)
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.messages import SystemMessage
from typing import List
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain.chains import LLMChain
from langchain_groq import ChatGroq
from collections import deque

class TranscriptionFormatter:
    def __init__(self, max_history_size: int = 10):
        self.model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.json_parser = StrOutputParser()
        self.history: List[str] = []

    async def run(self, transcription: str) -> str:
        print("Received Format request: {}".format(transcription))
        system_message = SystemMessage('''
            You are an expert context-aware transcription formatter that fixes the mistranscriptions done by a speech-to-text engine due to problems like accents and stutters.
                                       
            Your job is to format the transcriptions generated by a speech-to-text engine into a grammatically correct and coherent text.
            If the sentence is incomplete, make sure to only correct what is available and leave the rest as incomplete.
            
            If the previous transcriptions are critical to the context, combine them with the current transcription to provide a comprehensive transcription.
            
            Note: The formatted transcriptions will be fed into another model to generate a response.
                                       
            Examples: [                 
                Input: ""Well, I'm trying to to to uhmmm figure out uhmm how to what you might call it, how to extract information "
                Output: "Well, I am trying to figure out how to extract information from"
                                       
                TranscriptionHistory: ["Well", "I'm trying to figure out how to what you might call it", "how to prompt my model properly.", " Do do you think using multiple layers of elements Could "]
                Input: "Could  Do you think using multi  layers of LLMs  be useful?"
                Output: "Well, I am trying to figure out how to... what you might call it... how to prompt my model properly. Do you think using multiple layers of LLMs would be useful?"

                TranscriptionHistory: ["I have been thinking about computer science,"]
                Input: "What is commuter science?"
                Output: "I have been thinking about computer science, What is computer science?"
            ]
            
            Make sure to take into account the history of transcriptions to accurately match the terminology.
            Transcription History: {}
            
            Please return a formatted transcription that also includes the Transcription History.  
            String Output
        '''.format(list(self.history)))
        
        human_message_prompt = HumanMessagePromptTemplate.from_template("{transcript}")
        
        prompt = ChatPromptTemplate.from_messages([system_message, human_message_prompt])
          
        chain = prompt | self.model | self.json_parser
        result = await chain.ainvoke({"transcript": transcription})

        self.history.append(result)

        return result
    
    def clear_history(self):
        self.history.clear()
    
class ShouldRespondDecider:
    def __init__(self, transcription_history_len: int = 5) -> None:
        self.model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, model_kwargs={
            "response_format": { 
            "type": "json_object" 
            }
        })
        self.output_parser = JsonOutputParser()
        self.transcription_history = deque(maxlen= transcription_history_len)

    async def run(self, transcription: str) -> bool:
        system_message = SystemMessage('''
            You are an expert intention detector that is in charge of deciding if the user has requests for an AI's help.
            You will receive live audio transcriptions from a transcription engine and will decide if an AI needs to intervene.

            An AI will need to intervene if:
            1. User has asked a question (phrases like Right? Isn't it? are also questions or other similar phrases that seek validation)
            2. User messages that include greetings or farewells.
                                       
            Here is a list of the recent chat history, use it to make an educated decision:
            {}

            Please output a JSON with the key as "model_should_respond" and the value as a Python bool.                   
        '''.format(list(self.transcription_history)))

        human_message_prompt = HumanMessagePromptTemplate.from_template("{transcript}")
        prompt = ChatPromptTemplate.from_messages([system_message, human_message_prompt])

        chain = prompt | self.model | self.output_parser
        result = await chain.ainvoke({"transcript": transcription})

        self.transcription_history.append(transcription)
        
        return result["model_should_respond"]
    
    def clear_transcription_history(self):
        self.transcription_history.clear()

class ChatModel:
    def __init__(self) -> None:
        self.model = ChatOpenAI(
            model="gpt-4",
            streaming=True,
            temperature=0.5
        )

        # self.model = ChatGroq(model="llama2-70b-4096", temperature=0.5, streaming=True)
        
        system_message = SystemMessage('''
        As an LLM used in a voice chat environment, prioritize brevity and focus in your responses. 
        Avoid lengthy or verbose answers unless absolutely necessary. Your goal is to provide concise and relevant information that is 
        easy to understand and digest in a conversational setting.
    
        Please ensure that your responses are tailored to the context of the conversation and address the user's query directly. 
        Avoid unnecessary details or extraneous information that may detract from the clarity and effectiveness of your response.

        Remember, in a voice chat environment, clear and succinct communication is key to maintaining engagement and facilitating 
        smooth interactions. Keep your answers brief and to the point, unless additional context or explanation is required to fully 
        address the user's inquiry.
        ''')
                                   
        human_message_prompt = HumanMessagePromptTemplate.from_template("{message}")
        
        prompt = ChatPromptTemplate.from_messages([
            system_message,
            MessagesPlaceholder(variable_name="history"), 
            human_message_prompt
        ])
       
        self.chat_history = {}
        
        self.chain = prompt | self.model

        self.chain_with_memory = RunnableWithMessageHistory(
            self.chain,
            self.get_session_history,
            input_messages_key="message",
            history_messages_key="history"
        )
        
    async def run(self, message: str):
        response_stream = self.chain_with_memory.astream({"message": message}, config={"configurable": {"session_id": "abc123"}})
        async for partial_response in response_stream:
            yield partial_response.content
                
    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        if session_id not in self.chat_history:
            self.chat_history[session_id] = ChatMessageHistory()
        return self.chat_history[session_id]
            
class ModelResponseHandler:
    def __init__(self):
        self.model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.output_parser = JsonOutputParser()

        '''
, model_kwargs={
            "response_format": { 
            "type": "json_object" 
            }
        }
        '''
    
    async def run(self, model_response: List[str]) -> tuple[bool, str]:
        # print("Received a ModelResponseHandler.Run request for the tokens: {}".format(model_response))
        system_message = SystemMessage('''
            You have been assigned the task of assembling a stream of tokens generated by a Large Language Model (LLM) asynchronously. 
            Your objective is to stich together these tokens to allow for it to be an input into a Text-to-Speech (TTS) model.

            Your task involves monitoring the incoming stream of tokens and determining when you have accumulated enough tokens to form a complete sentence suitable for TTS input. 
            Once you have identified such a sentence, your system should return True along with the stitched-together tokens.

            Remove explicit displays of tokens like escape characters.
            Make sure to format the string to accomodate an output parser to be able to extract your response as a JSON object.
            Try not to end the result_text with the following structure "1." where the output parser might have trouble. 
            
            If you have deemed should_return_response to be False then return only a JSON with "should_return_response": <True or False>

            If you have deemed should_return_response to be True then return a JSON object with key-value mappings using:
            "should_return_response": <True or False>
            "response_text": String
        ''')
        human_message_prompt = HumanMessagePromptTemplate.from_template("{model_response}")

        prompt = ChatPromptTemplate.from_messages([system_message, human_message_prompt])
        
        chain: LLMChain = prompt | self.model | self.output_parser

        result = await chain.ainvoke({"model_response": model_response})
        should_return_response = result["should_return_response"]
        
        if should_return_response:
            return (should_return_response, result["response_text"])
        
        return (should_return_response, "")
        
        